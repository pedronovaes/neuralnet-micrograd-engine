{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libs and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.tensor import Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we are simulating how to compute the gradient of a function $f$ using **automatic differentiation** (a.k.a auto-diff), a set of techniques used to evaluate the function's partial derivative.\n",
    "\n",
    "In a few words, auto-diff exploits the fact every computer calculation executes a sequence of **elementary arithmetic operations** (addition, substraction, multiplication, division, etc) and **elementary functions** (exp, log, sin, cos, etc). All numeric computation is centered around these operations, and since we know their derivatives, we can chain them together (by applying the **chain rule** $[\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdotp \\frac{\\partial y}{\\partial x}]$) to arrive at the derivative for the entire function.\n",
    "\n",
    "The function $f$ we are simulating here is a simple neuron. This neuron has a shape of $$f(\\sum_{i}{w_i x_i + b})$$ where the function $f$ is an activation function (*tanh* for this demo), $x_i$ are the inputs, $w_i$ the weights, and $b$ the bias of the neuron.\n",
    "\n",
    "Let's define these variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: Value(data=0.881373, grad=0.0)\n",
      "o: Value(data=0.7071064876766542, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "x1 = Value(data=2.0)\n",
    "x2 = Value(data=0.0)\n",
    "\n",
    "# Weights\n",
    "w1 = Value(data=-3.0)\n",
    "w2 = Value(data=1.0)\n",
    "\n",
    "# Bias\n",
    "b = Value(data=6.881373)\n",
    "\n",
    "# Inside sum\n",
    "x1w1 = x1 * w1\n",
    "x2w2 = x2 * w2\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2\n",
    "\n",
    "# Adding with bias\n",
    "n = x1w1x2w2 + b\n",
    "print(f'n: {n}')\n",
    "\n",
    "# Tanh\n",
    "o = n.tanh()\n",
    "print(f'o: {o}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating derivatives manually\n",
    "\n",
    "First, we'll calculate the derivative of ou function $f$ manually. After that, we'll implement the backpropagation algorithm.\n",
    "\n",
    "The derivative of $o$ with respect to $o$ is `1.0` given that is the base case $(\\frac{\\partial o}{\\partial o})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o: Value(data=0.7071064876766542, grad=1.0)\n"
     ]
    }
   ],
   "source": [
    "o.grad = 1.0\n",
    "\n",
    "print(f'o: {o}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $o$ is connected to $n$ in our graph. So, the derivative of $o$ with respect to $n$ $(\\frac{\\partial o}{\\partial n})$ is $1 - tanh(n)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: Value(data=0.881373, grad=0.5000004150855857)\n"
     ]
    }
   ],
   "source": [
    "n.grad = 1 - o.data ** 2  # 0.5 given that o.data is the same of n.tanh()\n",
    "\n",
    "print(f'n: {n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the derivative of $o$ with respect to $x_1w_1x_2w_2$ $(\\frac{\\partial o}{\\partial x_1w_1x_2w_2})$ is necessary to use the **chain rule**:\n",
    "\n",
    "$$\\frac{\\partial o}{\\partial x_1w_1x_2w_2} = \\frac{\\partial o}{\\partial n} \\cdotp \\frac{\\partial n}{\\partial x_1w_1x_2w_2}$$\n",
    "\n",
    "We know that $\\frac{\\partial n}{\\partial x_1w_1x_2w_2}$ is equal to `1.0`. So, the $\\frac{\\partial o}{\\partial x_1w_1x_2w_2}$ is the same of $\\frac{\\partial o}{\\partial n}$ since a **plus operation** is just a distributor of gradient.\n",
    "\n",
    "The same applies to $\\frac{\\partial o}{\\partial b}$, $\\frac{\\partial o}{\\partial x_2w_2}$, and $\\frac{\\partial o}{\\partial x_1w_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1w1x2w2: Value(data=-6.0, grad=0.5000004150855857)\n",
      "b: Value(data=6.881373, grad=0.5000004150855857)\n",
      "x2w2: Value(data=0.0, grad=0.5000004150855857)\n",
      "x1w1: Value(data=-6.0, grad=0.5000004150855857)\n"
     ]
    }
   ],
   "source": [
    "x1w1x2w2.grad = n.grad\n",
    "b.grad = n.grad\n",
    "x2w2.grad = n.grad\n",
    "x1w1.grad = n.grad\n",
    "\n",
    "print(f'x1w1x2w2: {x1w1x2w2}')\n",
    "print(f'b: {b}')\n",
    "print(f'x2w2: {x2w2}')\n",
    "print(f'x1w1: {x1w1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multiplication case (such as the node $x_1w_1$ that is composed by the variables $x_1$ and $w_1$), we can assume that the local derivative of one of the variables is equal to the other data's variable. For example: $\\frac{\\partial x_1w_1}{\\partial x_1}$ is equal to $w_1$'s value and so on.\n",
    "\n",
    "By applying the chain rule, we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: Value(data=2.0, grad=-1.500001245256757)\n",
      "w1: Value(data=-3.0, grad=1.0000008301711714)\n",
      "x2: Value(data=0.0, grad=0.5000004150855857)\n",
      "w2: Value(data=1.0, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "x1.grad = w1.data * x1w1.grad\n",
    "w1.grad = x1.data * x1w1.grad\n",
    "\n",
    "x2.grad = w2.data * x2w2.grad\n",
    "w2.grad = x2.data * x2w2.grad\n",
    "\n",
    "print(f'x1: {x1}')\n",
    "print(f'w1: {w1}')\n",
    "print(f'x2: {x2}')\n",
    "print(f'w2: {w2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
