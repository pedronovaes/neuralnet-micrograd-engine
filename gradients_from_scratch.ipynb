{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libs and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.tensor import Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up gradient computation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this demo, we are simulating how to compute the gradient of a function $f$ using **automatic differentiation** (a.k.a auto-diff), a set of techniques used to evaluate the partial derivative of an function.\n",
    "\n",
    "In a few words, auto-diff exploits the fact every computer calculation executes a sequence of **elementary arithmetic operations** (such as addition, substraction, multiplication, division, etc) and **elementary functions** (such as exp, log, sin, cos, etc).\n",
    "\n",
    "All numeric computation is centered around these two kinds of operations, and since we know their derivative, we can chain them together (by applyind the **chain rule** $[\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdotp \\frac{\\partial y}{\\partial x}]$) to arrive at the derivative for the entire function.\n",
    "\n",
    "The function $f$ we are simulating here is a simple neuron. This neuron has a shape of $$f(\\sum_{i}{w_i x_i + b})$$ where the function $f$ is an activation function (*tanh* for this demo), $x_i$ are the inputs, $w_i$ the weights, and $b$ the bias of the neuron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's define all the variables used in our function $f$:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: Value(data=0.881373, grad=0.0)\n",
      "o: Value(data=0.7071064876766542, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "x1 = Value(data=2.0)\n",
    "x2 = Value(data=0.0)\n",
    "\n",
    "# Weights\n",
    "w1 = Value(data=-3.0)\n",
    "w2 = Value(data=1.0)\n",
    "\n",
    "# Bias\n",
    "b = Value(data=6.881373)\n",
    "\n",
    "# Inside sum\n",
    "x1w1 = x1 * w1\n",
    "x2w2 = x2 * w2\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2\n",
    "\n",
    "# Adding with bias\n",
    "n = x1w1x2w2 + b\n",
    "print(f'n: {n}')\n",
    "\n",
    "# Tanh\n",
    "o = n.tanh()\n",
    "print(f'o: {o}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating derivatives manually\n",
    "\n",
    "First, we'll calculate the derivative of ou function $f$ manually. After that, we'll implement the backpropagation algorithm.\n",
    "\n",
    "The derivative of $o$ with respect to $o$ is `1.0` given that is the base case $(\\frac{\\partial o}{\\partial o})$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o: Value(data=0.7071064876766542, grad=1.0)\n"
     ]
    }
   ],
   "source": [
    "o.grad = 1.0\n",
    "\n",
    "print(f'o: {o}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable $o$ is a hyperbolic tangent function of $n$ (i.e. it's connected to $n$ in our graph).\n",
    "\n",
    "Therefore, the derivative of $o$ with respect to $n$ ($\\frac{\\partial o}{\\partial n}$) is equal to $1 - tanh(n)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: Value(data=0.881373, grad=0.5000004150855857)\n"
     ]
    }
   ],
   "source": [
    "n.grad = 1 - o.data ** 2  # 0.5 given that o.data is the same of n.tanh()\n",
    "\n",
    "print(f'n: {n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To calculate the derivative of $o$ with respect to $x_1w_1x_2w_2$ $(\\frac{\\partial o}{\\partial x_1w_1x_2w_2})$ is necessary to use the **chain rule**:\n",
    "\n",
    "$$\\frac{\\partial o}{\\partial x_1w_1x_2w_2} = \\frac{\\partial o}{\\partial n} \\cdotp \\frac{\\partial n}{\\partial x_1w_1x_2w_2}$$\n",
    "\n",
    "We know that $\\frac{\\partial n}{\\partial x_1w_1x_2w_2}$ is equal to `1.0`. So, the $\\frac{\\partial o}{\\partial x_1w_1x_2w_2}$ is the same of $\\frac{\\partial o}{\\partial n}$ since a **plus operation** is just a distributor of gradient.\n",
    "\n",
    "The same applies to $\\frac{\\partial o}{\\partial b}$, $\\frac{\\partial o}{\\partial x_2w_2}$, and $\\frac{\\partial o}{\\partial x_1w_1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1w1x2w2: Value(data=-6.0, grad=0.5000004150855857)\n",
      "b: Value(data=6.881373, grad=0.5000004150855857)\n",
      "x2w2: Value(data=0.0, grad=0.5000004150855857)\n",
      "x1w1: Value(data=-6.0, grad=0.5000004150855857)\n"
     ]
    }
   ],
   "source": [
    "x1w1x2w2.grad = n.grad\n",
    "b.grad = n.grad\n",
    "x2w2.grad = n.grad\n",
    "x1w1.grad = n.grad\n",
    "\n",
    "print(f'x1w1x2w2: {x1w1x2w2}')\n",
    "print(f'b: {b}')\n",
    "print(f'x2w2: {x2w2}')\n",
    "print(f'x1w1: {x1w1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a multiplication case (such as the node $x_1w_1$ that is composed by the variables $x_1$ and $w_1$), we can assume the local derivative of one of the variables ($x_1$, for example) is equal to the other data's variable (by the derivative definition). Therefore, in this local example, the derivative of $x_1w_1$ with respect of $x_1$ $(\\frac{\\partial x_1w_1}{\\partial x_1})$ is equal to $w_1$'s value and so on.\n",
    "\n",
    "To calculate the derivative of $o$ with respect to $x_1$ $(\\frac{\\partial o}{\\partial x_1})$ is also necessary to use the **chain rule**:\n",
    "\n",
    "$$\\frac{\\partial o}{\\partial x_1} = \\frac{\\partial o}{\\partial x_1w_1} \\cdotp \\frac{\\partial x_1w_1}{x_1}$$\n",
    "\n",
    "The result of above chain rule is equal to $x_1w_1$ gradient $\\times$ $w_1$ data. And we can extend the same logic to compute $\\frac{\\partial o}{\\partial w_1}$, $\\frac{\\partial o}{\\partial x_2}$, and $\\frac{\\partial o}{\\partial w_2}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: Value(data=2.0, grad=-1.500001245256757)\n",
      "w1: Value(data=-3.0, grad=1.0000008301711714)\n",
      "x2: Value(data=0.0, grad=0.5000004150855857)\n",
      "w2: Value(data=1.0, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "x1.grad = w1.data * x1w1.grad\n",
    "w1.grad = x1.data * x1w1.grad\n",
    "\n",
    "x2.grad = w2.data * x2w2.grad\n",
    "w2.grad = x2.data * x2w2.grad\n",
    "\n",
    "print(f'x1: {x1}')\n",
    "print(f'w1: {w1}')\n",
    "print(f'x2: {x2}')\n",
    "print(f'w2: {w2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating derivatives using backward function\n",
    "\n",
    "We need to redefine all the variables to calculate the derivatives using backward in the right order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: Value(data=0.881373, grad=0.0)\n",
      "o: Value(data=0.7071064876766542, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "x1 = Value(data=2.0)\n",
    "x2 = Value(data=0.0)\n",
    "\n",
    "# Weights\n",
    "w1 = Value(data=-3.0)\n",
    "w2 = Value(data=1.0)\n",
    "\n",
    "# Bias\n",
    "b = Value(data=6.881373)\n",
    "\n",
    "# Inside sum\n",
    "x1w1 = x1 * w1\n",
    "x2w2 = x2 * w2\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2\n",
    "\n",
    "# Adding with bias\n",
    "n = x1w1x2w2 + b\n",
    "print(f'n: {n}')\n",
    "\n",
    "# Tanh\n",
    "o = n.tanh()\n",
    "print(f'o: {o}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the base case, we need to set the grad to $1.0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: Value(data=0.881373, grad=0.5000004150855857)\n"
     ]
    }
   ],
   "source": [
    "o.grad = 1.0\n",
    "o._backward() # This code should propagates the gradient through tanh\n",
    "\n",
    "print(f'n: {n}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1w1x2w2: Value(data=-6.0, grad=0.5000004150855857)\n",
      "b: Value(data=6.881373, grad=0.5000004150855857)\n"
     ]
    }
   ],
   "source": [
    "n._backward() # Propagates the gradient to x1w1x2w2 and b\n",
    "\n",
    "print(f'x1w1x2w2: {x1w1x2w2}')\n",
    "print(f'b: {b}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1w1: Value(data=-6.0, grad=0.5000004150855857)\n",
      "x2w2: Value(data=0.0, grad=0.5000004150855857)\n"
     ]
    }
   ],
   "source": [
    "x1w1x2w2._backward() # Propagates the gradient to x1w1 and x2w2\n",
    "\n",
    "print(f'x1w1: {x1w1}')\n",
    "print(f'x2w2: {x2w2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1: Value(data=2.0, grad=-1.500001245256757)\n",
      "w1: Value(data=-3.0, grad=1.0000008301711714)\n",
      "x2: Value(data=0.0, grad=0.5000004150855857)\n",
      "w2: Value(data=1.0, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "x1w1._backward() # Propagates the gradient to x1 and w1\n",
    "x2w2._backward() # Propagates the gradient to x2 and w2\n",
    "\n",
    "print(f'x1: {x1}')\n",
    "print(f'w1: {w1}')\n",
    "print(f'x2: {x2}')\n",
    "print(f'w2: {w2}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating derivatives from last node of graph\n",
    "\n",
    "Let's redefine all the variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: Value(data=0.881373, grad=0.0)\n",
      "o: Value(data=0.7071064876766542, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "x1 = Value(data=2.0)\n",
    "x2 = Value(data=0.0)\n",
    "\n",
    "# Weights\n",
    "w1 = Value(data=-3.0)\n",
    "w2 = Value(data=1.0)\n",
    "\n",
    "# Bias\n",
    "b = Value(data=6.881373)\n",
    "\n",
    "# Inside sum\n",
    "x1w1 = x1 * w1\n",
    "x2w2 = x2 * w2\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2\n",
    "\n",
    "# Adding with bias\n",
    "n = x1w1x2w2 + b\n",
    "print(f'n: {n}')\n",
    "\n",
    "# Tanh\n",
    "o = n.tanh()\n",
    "print(f'o: {o}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "o.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: Value(data=0.881373, grad=0.5000004150855857)\n",
      "\n",
      "b: Value(data=6.881373, grad=0.5000004150855857)\n",
      "x1w1x2w2: Value(data=-6.0, grad=0.5000004150855857)\n",
      "\n",
      "x1w1: Value(data=-6.0, grad=0.5000004150855857)\n",
      "x2w2: Value(data=0.0, grad=0.5000004150855857)\n",
      "\n",
      "x1: Value(data=2.0, grad=-1.500001245256757)\n",
      "w1: Value(data=-3.0, grad=1.0000008301711714)\n",
      "x2: Value(data=0.0, grad=0.5000004150855857)\n",
      "w2: Value(data=1.0, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "print(f'n: {n}')\n",
    "\n",
    "print()\n",
    "print(f'b: {b}')\n",
    "print(f'x1w1x2w2: {x1w1x2w2}')\n",
    "\n",
    "print()\n",
    "print(f'x1w1: {x1w1}')\n",
    "print(f'x2w2: {x2w2}')\n",
    "\n",
    "print()\n",
    "print(f'x1: {x1}')\n",
    "print(f'w1: {w1}')\n",
    "print(f'x2: {x2}')\n",
    "print(f'w2: {w2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
