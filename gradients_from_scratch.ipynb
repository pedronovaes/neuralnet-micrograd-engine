{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Libs and configs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from micrograd.tensor import Value"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setting up gradient computation\n",
    "\n",
    "In this demo, we are simulating how to compute the gradient of a function using **automatic differentiation** (a.k.a auto-diff), a set of techniques used to evaluate the function partial derivative.\n",
    "\n",
    "In a few words, the auto-diff exploits the fact every computer calculation executes a sequence of elementary arithmetic operations (addition, substraction, multiplication, division, etc) and elementary functions (exp, log, sin, cos, etc). By applying the **chain rule** $(\\frac{\\partial z}{\\partial x} = \\frac{\\partial z}{\\partial y} \\cdotp \\frac{\\partial y}{\\partial x})$ reoeatedly to these operations, partial derivatives can be computer automatically.\n",
    "\n",
    "The function we are simulating here is a simple neuron. This neuron has a shape of $$f(\\sum_{i}{w_i x_i + b})$$ where the function $f$ is an activation function (*tanh* for this demo), $x_i$ are the inputs, $w_i$ the weights, and $b$ the bias of the neuron."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: Value(data=0.881373, grad=0.0)\n",
      "o: Value(data=0.7071064876766542, grad=0.0)\n"
     ]
    }
   ],
   "source": [
    "# Inputs\n",
    "x1 = Value(data=2.0)\n",
    "x2 = Value(data=0.0)\n",
    "\n",
    "# Weights\n",
    "w1 = Value(data=-3.0)\n",
    "w2 = Value(data=1.0)\n",
    "\n",
    "# Bias\n",
    "b = Value(data=6.881373)\n",
    "\n",
    "# Inside sum\n",
    "x1w1 = x1 * w1\n",
    "x2w2 = x2 * w2\n",
    "\n",
    "x1w1x2w2 = x1w1 + x2w2\n",
    "\n",
    "n = x1w1x2w2 + b\n",
    "print(f'n: {n}')\n",
    "\n",
    "# Tanh\n",
    "o = n.tanh()\n",
    "print(f'o: {o}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculating derivatives manually"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of $o$ with respect to $o$ (base case $\\frac{\\partial o}{\\partial o}$) is 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "o: Value(data=0.7071064876766542, grad=1.0)\n"
     ]
    }
   ],
   "source": [
    "o.grad = 1.0\n",
    "\n",
    "print(f'o: {o}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The derivative of o with respect to n $\\frac{\\partial o}{\\partial n}$ is $1 - tanh(n)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n: Value(data=0.881373, grad=0.5000004150855857)\n"
     ]
    }
   ],
   "source": [
    "n.grad = 1 - o.data ** 2  # 0.5\n",
    "\n",
    "print(f'n: {n}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Derivative of o with respect of x1w1x2w2 $\\frac{\\partial o}{\\partial x1w1x2w2}$ is the same as $\\frac{\\partial o}{\\partial n}$ since a plus operation is just a distributor of gradient (as well as gradients $\\frac{\\partial o}{\\partial b}$, $\\frac{\\partial o}{\\partial x2w2}$, and $\\frac{\\partial o}{\\partial x1w1}$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x1w1x2w2: Value(data=-6.0, grad=0.5000004150855857)\n",
      "b: Value(data=6.881373, grad=0.5000004150855857)\n",
      "x2w2: Value(data=0.0, grad=0.5000004150855857)\n",
      "x1w1: Value(data=-6.0, grad=0.5000004150855857)\n"
     ]
    }
   ],
   "source": [
    "x1w1x2w2.grad = n.grad\n",
    "b.grad = n.grad\n",
    "x2w2.grad = n.grad\n",
    "x1w1.grad = n.grad\n",
    "\n",
    "print(f'x1w1x2w2: {x1w1x2w2}')\n",
    "print(f'b: {b}')\n",
    "print(f'x2w2: {x2w2}')\n",
    "print(f'x1w1: {x1w1}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "playground",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
